# MCTS-LLM Alpha Mining Framework

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.8+](https://img.shields.io/badge/python-3.8+-blue.svg)](https://www.python.org/downloads/)

An advanced quantitative alpha factor mining framework that combines Monte Carlo Tree Search (MCTS) with Large Language Models (LLMs) to discover formulaic alpha factors for stock prediction.

## ğŸŒŸ Overview

This project implements the cutting-edge algorithm described in the paper "Formulaic Alpha Mining using Monte Carlo Tree Search with LLM". It uses GPT-4 to generate and refine mathematical formulas that predict stock returns, evaluating them across multiple quality dimensions:

- **Effectiveness**: Signal strength and prediction accuracy
- **Stability**: Robustness across market conditions
- **Turnover**: Trading frequency (lower is better)
- **Diversity**: Uniqueness from existing factors
- **Overfitting**: Generalization capability

## ğŸš€ Key Features

- **Two-Step Generation Process**: High-level alpha portrait â†’ Symbolic formula with parameters
- **Multi-dimensional Evaluation**: Comprehensive assessment across 5 quality dimensions
- **Virtual Action Mechanism**: Enables expansion of internal nodes in MCTS
- **Relative Ranking System**: Dynamic scoring based on repository performance
- **Few-shot Learning**: Context-aware formula refinement using successful examples
- **Real Market Data**: Integration with Qlib for CSI300 universe evaluation

## ğŸ“‹ Requirements

- Python 3.8+
- OpenAI API key (GPT-4 access)
- Qlib with CSI300 market data
- CUDA-capable GPU (optional, for faster Qlib computations)

## ğŸ› ï¸ Installation

1. **Clone the repository**
```bash
git clone https://github.com/dtbtc/mcts-llm-alpha.git
cd mcts-llm-alpha
```

2. **Set up Python environment**
```bash
conda create -n alphagen_dev python=3.8
conda activate alphagen_dev
```

3. **Install the package**
```bash
cd mcts-llm-alpha
pip install -e ".[dev]"
```

4. **Set up environment variables**
```bash
cp .env.example .env
# Edit .env to add your OPENAI_API_KEY and QLIB_PROVIDER_URI
```

5. **Download Qlib data (CSI300)**
```bash
python -m qlib.run.get_data qlib_data --target_dir ./qlib_data/cn_data --region cn
```

## ğŸ¯ Quick Start

### Basic MCTS Search
```bash
# Run with default settings (50 iterations)
mcts-llm-alpha search --iterations 50

# Run with custom configuration
mcts-llm-alpha search --config config/experiment.yaml

# Resume from checkpoint
mcts-llm-alpha resume --checkpoint checkpoints/mcts_checkpoint_20240101_120000.pkl
```

### Direct Python Script
```bash
cd mcts-llm-alpha
python run_search.py --iterations 10
```

### Jupyter Notebook
```bash
jupyter notebook notebooks/demo.ipynb
```

## ğŸ“Š Example Output

```
[5] å¼€å§‹MCTSæœç´¢...
============================================================

è®­ç»ƒè¿‡ç¨‹è¯´æ˜:
  1. LLMç”Ÿæˆåˆå§‹Alphaç”»åƒï¼ˆæè¿°ï¼‰
  2. å°†ç”»åƒè½¬æ¢ä¸ºç¬¦å·å…¬å¼
  3. è¯„ä¼°å¤šç»„å‚æ•°ï¼Œé€‰æ‹©æœ€ä¼˜
  4. å¼€å§‹MCTSæ ‘æœç´¢:
     - é€‰æ‹©ï¼šä½¿ç”¨UCTç®—æ³•é€‰æ‹©èŠ‚ç‚¹
     - æ‰©å±•ï¼šLLMé’ˆå¯¹ç‰¹å®šç»´åº¦ä¼˜åŒ–å…¬å¼
     - è¯„ä¼°ï¼šè®¡ç®—5ä¸ªç»´åº¦çš„å¾—åˆ†
     - å›ä¼ ï¼šæ›´æ–°æ ‘èŠ‚ç‚¹ç»Ÿè®¡

============================================================

ğŸ¯ æœ€ä½³å‘ç°å…¬å¼:
å…¬å¼: Rank((Mean($volume, 5) / Mean($volume, 20)) * Corr($close, $volume, 15), 10)
æœ€ä½³åˆ†æ•°: 6.842

ğŸ“š Alphaä»“åº“ç»Ÿè®¡:
å…¥åº“å› å­æ•°: 12
ä»“åº“å¹³å‡åˆ†: 5.73
ä»“åº“æœ€é«˜åˆ†: 6.84
```

## ğŸ—ï¸ Architecture

```
mcts-llm-alpha/
â”œâ”€â”€ src/mcts_llm_alpha/
â”‚   â”œâ”€â”€ config/          # Configuration management
â”‚   â”œâ”€â”€ llm/            # LLM integration (OpenAI, prompts)
â”‚   â”œâ”€â”€ mcts/           # Core MCTS algorithm
â”‚   â”œâ”€â”€ formula/        # Formula processing pipeline
â”‚   â”œâ”€â”€ evaluation/     # Multi-dimensional evaluation
â”‚   â”œâ”€â”€ data/           # Qlib data provider wrapper
â”‚   â””â”€â”€ cli.py          # Command-line interface
â”œâ”€â”€ tests/              # Comprehensive test suite
â”œâ”€â”€ notebooks/          # Example notebooks
â””â”€â”€ docs/              # Documentation
```

## ğŸ“ˆ Performance & Results

The system has been tested on CSI300 universe with the following improvements:
- Fixed relative ranking mechanism to correctly implement paper's formula
- Enhanced formula diversity through structural changes (not just parameters)
- Improved cold-start handling with progressive threshold adjustment
- Optimized caching for 10x faster repeated evaluations

## ğŸ”§ Configuration

Key parameters in `config/default.yaml`:
- `max_iterations`: Total MCTS search iterations (default: 50)
- `exploration_constant`: UCT exploration vs exploitation (default: 1.0)
- `dimension_temperature`: Softmax temperature for dimension selection (default: 10.0)
- `effectiveness_threshold`: Minimum effectiveness score (default: 3.0)
- `diversity_threshold`: Minimum diversity score (default: 2.0)

## ğŸ§ª Testing

```bash
# Run all tests
pytest

# Run with coverage
pytest -v --cov=mcts_llm_alpha --cov-report=html

# Run specific test
pytest tests/test_mcts_core.py::test_node_selection
```

## ğŸ“š Documentation

- [Architecture Overview](mcts-llm-alpha/docs/architecture.md)
- [MCTS Algorithm Details](Artical/MTCS_LLM_Alpha.md)
- [ä¸­æ–‡æ–‡æ¡£](Artical/alpha_mtcs_cn.md)
- [Fix History](mcts-llm-alpha/docs/MCTS_LLMALPHA_FIX_PLAN.md)

## ğŸ¤ Contributing

Contributions are welcome! Please feel free to submit a Pull Request.

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- Based on the paper "Formulaic Alpha Mining using Monte Carlo Tree Search with LLM"
- Uses [Qlib](https://github.com/microsoft/qlib) for market data and backtesting
- Powered by OpenAI's GPT-4 for formula generation

## ğŸ“§ Contact

For questions or suggestions, please open an issue on GitHub.

---

**Note**: This system requires real market data and OpenAI API access. Simulation mode has been removed to ensure authentic alpha factor discovery.