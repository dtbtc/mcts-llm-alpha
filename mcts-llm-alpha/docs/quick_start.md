# Quick Start Guide

## Prerequisites

Before starting, ensure you have:
- Python 3.8+ installed
- OpenAI API key with GPT-4 access
- At least 16GB RAM
- 10GB free disk space

## 1. Installation (5 minutes)

### Clone and Install
```bash
# Clone the repository
git clone https://github.com/dtbtc/mcts-llm-alpha.git
cd mcts-llm-alpha

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install package
cd mcts-llm-alpha
pip install -e ".[dev]"
```

### Set Up Environment
```bash
# Copy example environment file
cp .env.example .env

# Edit .env and add your OpenAI API key
# OPENAI_API_KEY=sk-your-actual-key-here
```

## 2. Download Market Data (10 minutes)

```bash
# Install Qlib
pip install qlib

# Download CSI300 data (Chinese market)
python -m qlib.run.get_data qlib_data --target_dir ./qlib_data --region cn

# Set data path in .env
# QLIB_PROVIDER_URI=/absolute/path/to/qlib_data
```

## 3. Your First Alpha Search (5 minutes)

### Basic Search
```bash
# Run a quick 10-iteration search
python run_search.py --iterations 10
```

### What You'll See
```
============================================================
MCTS-LLM AlphaæŒ–æ˜ç³»ç»Ÿ
============================================================

[1] åŠ è½½é…ç½®...
  - æœ€å¤§è¿­ä»£æ¬¡æ•°: 10
  - è¯„ä¼°æ¨¡å¼: QlibçœŸå®è¯„ä¼° + ç›¸å¯¹æ’å
  - LLMæ¨¡å‹: gpt-4o-mini

[2] åˆå§‹åŒ–æ•°æ®æä¾›è€…...
  - Qlibåˆå§‹åŒ–æˆåŠŸ

[3] åˆå§‹åŒ–LLMå®¢æˆ·ç«¯...
  - æ‰¾åˆ°APIå¯†é’¥ï¼Œä½¿ç”¨çœŸå®LLM

[4] åˆ›å»ºMCTSæœç´¢å®ä¾‹...
  - MCTSæœç´¢å®ä¾‹åˆ›å»ºå®Œæˆ

[5] å¼€å§‹MCTSæœç´¢...

ã€ç¬¬1æ­¥ã€‘LLMç”ŸæˆAlphaç”»åƒ...
ç”Ÿæˆçš„Alphaç”»åƒ:
----------------------------------------
### Alpha Factor Portrait

**Alpha Name:** Momentum Volatility Adjusted

**Description:** Captures price momentum normalized by recent volatility

**Formula Logic:**
```
momentum = (close - close[20]) / close[20]
volatility = std(returns, 20)
signal = momentum / volatility
alpha = rank(signal)
```
----------------------------------------

ã€ç¬¬2æ­¥ã€‘å°†ç”»åƒè½¬æ¢ä¸ºç¬¦å·å…¬å¼...
ç¬¦å·å…¬å¼: Rank((($close - Ref($close, w1)) / Ref($close, w1)) / Std(Delta($close, 1), w2), w3)

ã€ç¬¬3æ­¥ã€‘è¯„ä¼°å€™é€‰å‚æ•°ç»„...
è¯„ä¼°å‚æ•°ç»„1:
  å‚æ•°: {'w1': 20, 'w2': 20, 'w3': 10}
  è¯„åˆ†è¯¦æƒ…:
    - Effectiveness: 6.82
    - Stability: 7.45
    - Turnover: 8.23
    - Diversity: 9.50
    - Overfitting: 5.00
  å¹³å‡åˆ†: 7.40
```

## 4. Understanding the Output

### Key Metrics
- **Effectiveness (æœ‰æ•ˆæ€§)**: Signal strength (IC, ICIR)
- **Stability (ç¨³å®šæ€§)**: Consistency over time
- **Turnover (æ¢æ‰‹ç‡)**: Trading frequency (higher score = lower turnover)
- **Diversity (å¤šæ ·æ€§)**: Uniqueness from existing factors
- **Overfitting (è¿‡æ‹Ÿåˆ)**: Generalization ability

### Success Criteria
- Overall score > 5.0
- Effectiveness > 3.0
- Diversity > 2.0

## 5. Common Use Cases

### A. Exploratory Search
```bash
# High exploration, quick iterations
python run_search.py --iterations 20 --config config/exploration.yaml
```

### B. Production Search
```bash
# Thorough search with checkpointing
python run_search.py --iterations 100 --checkpoint-freq 10
```

### C. Resume from Checkpoint
```bash
# Continue a previous search
python -m mcts_llm_alpha resume --checkpoint checkpoints/mcts_checkpoint_20240101_120000.pkl
```

### D. With Seed Formula
```bash
# Start with a known good formula
python run_search.py --seed-formula "Rank(($close - Mean($close, 20)) / Std($close, 20), 10)"
```

## 6. Monitoring Progress

### Log Files
```bash
# Real-time log monitoring
tail -f logs/mcts_llm_alpha_latest.log

# Search for successful discoveries
grep "å…¥åº“Alpha" logs/mcts_llm_alpha_latest.log
```

### Checkpoints
```bash
# List checkpoints
ls -la checkpoints/

# Analyze checkpoint
python -m mcts_llm_alpha analyze --checkpoint checkpoints/latest.pkl
```

## 7. Quick Tips

### Memory Issues?
```yaml
# Reduce batch size in config
evaluation:
  batch_size: 500  # From 1000
  cache_size: 5000  # From 10000
```

### Slow Evaluation?
```yaml
# Increase parallelism
evaluation:
  n_jobs: -1  # Use all CPU cores
```

### Want Better Quality?
```yaml
# Use better model and stricter thresholds
llm:
  model: "gpt-4"
  
mcts:
  effectiveness_threshold: 4.0
  diversity_threshold: 3.0
```

## 8. Next Steps

1. **Read the Docs**
   - [Configuration Guide](configuration.md) - Customize your search
   - [Evaluation System](evaluation_system.md) - Understand scoring
   - [Architecture](architecture.md) - Deep dive into design

2. **Experiment**
   - Try different seed formulas
   - Adjust exploration parameters
   - Create custom configurations

3. **Analyze Results**
   - Review discovered factors in `results/`
   - Backtest top factors in your trading system
   - Compare with existing factors

## 9. Troubleshooting

### "No module named 'qlib'"
```bash
pip install qlib
```

### "OPENAI_API_KEY not found"
```bash
# Linux/Mac
export OPENAI_API_KEY=sk-your-key

# Windows
set OPENAI_API_KEY=sk-your-key
```

### "Qlib data not found"
```bash
# Download data first
python -m qlib.run.get_data qlib_data --target_dir ./qlib_data --region cn

# Update .env
QLIB_PROVIDER_URI=/absolute/path/to/qlib_data
```

### Getting Help
- Check [Common Issues](common_issues.md)
- Open an [issue on GitHub](https://github.com/dtbtc/mcts-llm-alpha/issues)
- Review the [FAQ](faq.md)

## 10. Example Results

After a successful run, you'll see:
```
æœç´¢å®Œæˆï¼
============================================================

ğŸ“Š åˆå§‹å…¬å¼ä¿¡æ¯:
å…¬å¼: Rank((($close - Ref($close, 20)) / Std($close, 20)), 10)
æ•´ä½“åˆ†æ•°: 6.54

ğŸ¯ æœ€ä½³å‘ç°å…¬å¼:
å…¬å¼: Rank((Mean($volume, 5) / Mean($volume, 20)) * Corr($close, $volume, 15), 10)
æœ€ä½³åˆ†æ•°: 7.842

ğŸ“š Alphaä»“åº“ç»Ÿè®¡:
å…¥åº“å› å­æ•°: 12
ä»“åº“å¹³å‡åˆ†: 6.73
```

Congratulations! You've discovered your first alpha factors! ğŸ‰